{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4024cea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-18 15:38:38.743497: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: SSE4.1 SSE4.2 AVX AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(13)\n",
    "tf.random.set_seed(13)\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
    "from wordcloud import WordCloud\n",
    "from xml.sax import ContentHandler, parse\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "\n",
    "#nltk.download('punkt')\n",
    "\n",
    "stemmer = SnowballStemmer('english', ignore_stopwords=True)\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87433104",
   "metadata": {},
   "source": [
    "### Class that hadles excel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99ed386f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 16 µs, sys: 2 µs, total: 18 µs\n",
      "Wall time: 20 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "class ExcelHandler(ContentHandler):\n",
    "    def __init__(self):\n",
    "        self.chars = [  ]\n",
    "        self.cells = [  ]\n",
    "        self.rows = [  ]\n",
    "        self.tables = [  ]\n",
    "    def characters(self, content):\n",
    "        self.chars.append(content)\n",
    "    def startElement(self, name, atts):\n",
    "        if name==\"Cell\":\n",
    "            self.chars = [  ]\n",
    "        elif name==\"Row\":\n",
    "            self.cells=[  ]\n",
    "        elif name==\"Table\":\n",
    "            self.rows = [  ]\n",
    "    def endElement(self, name):\n",
    "        if name==\"Cell\":\n",
    "            self.cells.append(''.join(self.chars))\n",
    "        elif name==\"Row\":\n",
    "            self.rows.append(self.cells)\n",
    "        elif name==\"Table\":\n",
    "            self.tables.append(self.rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "245ea807",
   "metadata": {},
   "outputs": [],
   "source": [
    "excelHandler = ExcelHandler()\n",
    "parse('data/features.xls', excelHandler)\n",
    "features = pd.DataFrame(excelHandler.tables[0][1:], columns=excelHandler.tables[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea81001",
   "metadata": {},
   "source": [
    "Parse Excel file and create dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39a1d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.where(features['Label'] == 'objective', 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aab7bf",
   "metadata": {},
   "source": [
    "Create labels: objective = 0, subjective = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ef60043",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "normalized_texts = []\n",
    "\n",
    "for i in range(1, 1001):\n",
    "    if i // 10 == 0:\n",
    "        num = '000' + str(i)\n",
    "    elif i // 100 == 0:\n",
    "        num = '00' + str(i)\n",
    "    elif i // 1000 == 0:\n",
    "        num = '0' + str(i)\n",
    "    else:\n",
    "        num = '1000'\n",
    "    \n",
    "    f = open('data/raw-data/Text' + num + '.txt', 'r', encoding='latin-1')\n",
    "    text = f.read()\n",
    "    \n",
    "    # removes any non-alphabetic characters and tokenizes \n",
    "    # the text from the Natural Language Toolkit (nltk)\n",
    "    \n",
    "    normalized_text = ' '.join([stemmer.stem(w) for w in word_tokenize(text) if (w.isalpha() and w not in stop)])\n",
    "    texts.append(text)\n",
    "    normalized_texts.append(normalized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fe7cc1",
   "metadata": {},
   "source": [
    "Read text files and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db7f3008",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame({'texts': np.array(texts), 'normalized_texts': np.array(normalized_texts), 'label': y})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8903f6d4",
   "metadata": {},
   "source": [
    "Create dataframe for the texts and their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a76edee",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_texts = ' '.join(dataframe[dataframe['label'] == 0]['normalized_texts'].tolist())\n",
    "sub_texts = ' '.join(dataframe[dataframe['label'] == 1]['normalized_texts'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac20ba1c",
   "metadata": {},
   "source": [
    "Create two strings for the preprocessed texts: one for objective and one for subjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74789879",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(normalized_texts), y, random_state=13, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1394d616",
   "metadata": {},
   "source": [
    "Splits the data into training and testing sets for use in a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1536f9",
   "metadata": {},
   "source": [
    "# Decision Tree's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0267b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0482c1eb",
   "metadata": {},
   "source": [
    "Create an instance of the TfidfVectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68f57a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e13d81",
   "metadata": {},
   "source": [
    "Fit the vectorizer on the training data and transform the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2871ac9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_vectorized = vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddca4336",
   "metadata": {},
   "source": [
    "Transform the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d43d2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_vectorized = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec3dd7f",
   "metadata": {},
   "source": [
    "Create an instance of the DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31ad2b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fa9dac",
   "metadata": {},
   "source": [
    "Fit the model on the vectorized training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "715eccdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DecisionTreeClassifier()"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(X_train_vectorized, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a27708",
   "metadata": {},
   "source": [
    "Make predictions on the vectorized test data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "087af2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test_vectorized)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f945005d",
   "metadata": {},
   "source": [
    "Summary of the predictions made by the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0f96c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.78      0.79       159\n",
      "           1       0.64      0.68      0.66        91\n",
      "\n",
      "    accuracy                           0.74       250\n",
      "   macro avg       0.72      0.73      0.73       250\n",
      "weighted avg       0.75      0.74      0.75       250\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70472d23",
   "metadata": {},
   "source": [
    "#### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9deee5f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[124  35]\n",
      " [ 29  62]]\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcb1b5f",
   "metadata": {},
   "source": [
    "#### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0086858b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy is 0.744\n"
     ]
    }
   ],
   "source": [
    "print('accuracy is',accuracy_score(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90530294",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069c1966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
