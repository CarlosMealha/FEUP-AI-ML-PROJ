{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4024cea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/carlos/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import time\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(13)\n",
    "tf.random.set_seed(13)\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold, train_test_split\n",
    "from wordcloud import WordCloud\n",
    "from xml.sax import ContentHandler, parse\n",
    "from zipfile import ZipFile\n",
    "\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras_preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from nltk import word_tokenize\n",
    "import nltk\n",
    "\n",
    "stemmer = SnowballStemmer('english', ignore_stopwords=True)\n",
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set(rc={'figure.figsize':(11.7,8.27)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87433104",
   "metadata": {},
   "source": [
    "### Class that hadles excel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "99ed386f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17 µs, sys: 0 ns, total: 17 µs\n",
      "Wall time: 20.3 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "class ExcelHandler(ContentHandler):\n",
    "    def __init__(self):\n",
    "        self.chars = [  ]\n",
    "        self.cells = [  ]\n",
    "        self.rows = [  ]\n",
    "        self.tables = [  ]\n",
    "    def characters(self, content):\n",
    "        self.chars.append(content)\n",
    "    def startElement(self, name, atts):\n",
    "        if name==\"Cell\":\n",
    "            self.chars = [  ]\n",
    "        elif name==\"Row\":\n",
    "            self.cells=[  ]\n",
    "        elif name==\"Table\":\n",
    "            self.rows = [  ]\n",
    "    def endElement(self, name):\n",
    "        if name==\"Cell\":\n",
    "            self.cells.append(''.join(self.chars))\n",
    "        elif name==\"Row\":\n",
    "            self.rows.append(self.cells)\n",
    "        elif name==\"Table\":\n",
    "            self.tables.append(self.rows)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "245ea807",
   "metadata": {},
   "outputs": [],
   "source": [
    "excelHandler = ExcelHandler()\n",
    "parse('data/features.xls', excelHandler)\n",
    "features = pd.DataFrame(excelHandler.tables[0][1:], columns=excelHandler.tables[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea81001",
   "metadata": {},
   "source": [
    "Parse Excel file and create dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "39a1d695",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.where(features['Label'] == 'objective', 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aab7bf",
   "metadata": {},
   "source": [
    "Create labels: objective = 0, subjective = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ef60043",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "normalized_texts = []\n",
    "\n",
    "for i in range(1, 1001):\n",
    "    if i // 10 == 0:\n",
    "        num = '000' + str(i)\n",
    "    elif i // 100 == 0:\n",
    "        num = '00' + str(i)\n",
    "    elif i // 1000 == 0:\n",
    "        num = '0' + str(i)\n",
    "    else:\n",
    "        num = '1000'\n",
    "    \n",
    "    f = open('data/raw-data/Text' + num + '.txt', 'r', encoding='latin-1')\n",
    "    text = f.read()\n",
    "    \n",
    "    # removes any non-alphabetic characters and tokenizes \n",
    "    # the text from the Natural Language Toolkit (nltk)\n",
    "    \n",
    "    normalized_text = ' '.join([stemmer.stem(w) for w in word_tokenize(text) if (w.isalpha() and w not in stop)])\n",
    "    texts.append(text)\n",
    "    normalized_texts.append(normalized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fe7cc1",
   "metadata": {},
   "source": [
    "Read text files and preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "db7f3008",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.DataFrame({'texts': np.array(texts), 'normalized_texts': np.array(normalized_texts), 'label': y})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8903f6d4",
   "metadata": {},
   "source": [
    "Create dataframe for the texts and their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6a76edee",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_texts = ' '.join(dataframe[dataframe['label'] == 0]['normalized_texts'].tolist())\n",
    "sub_texts = ' '.join(dataframe[dataframe['label'] == 1]['normalized_texts'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac20ba1c",
   "metadata": {},
   "source": [
    "Create two strings for the preprocessed texts: one for objective and one for subjective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74789879",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(np.array(normalized_texts), y, random_state=13, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1394d616",
   "metadata": {},
   "source": [
    "Splits the data into training and testing sets for use in a machine learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "501d082c",
   "metadata": {},
   "source": [
    "##### Pre-trained Glove (Global Vectors for Word Representation) embeddings derived from 6 billion tokens to prepare the embeddings for training our deep learning models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "88e2c5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 0 ns, total: 3 µs\n",
      "Wall time: 5.48 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "vector_size = 50 # how big is each word vector\n",
    "unique_words = 12000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "nwords = 200 # max number of words in text to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59492794",
   "metadata": {},
   "source": [
    "Decision variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3f6b2623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a Tokenizer object that will tokenize and vectorize the data\n",
    "\n",
    "token = Tokenizer(num_words=unique_words)\n",
    "\n",
    "# Updates the internal vocabulary of the tokenizer based on the training data \n",
    "\n",
    "token.fit_on_texts(X_train)\n",
    "\n",
    "# Tokenizes and converts the training data (X_train) into a list of \n",
    "# sequences of integers where each integer represents a specific word in the vocabulary.\n",
    "\n",
    "token_trainer = tokenizer.texts_to_sequences(X_train)\n",
    "\n",
    "# Tokenizes and converts the test data (X_test) into a list of sequences of integers where each integer represents a specific word in the vocabulary\n",
    "\n",
    "token_tester = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pads or truncates each sequence of integers to a fixed length of nwords\n",
    "# and returns a numpy array of shape (num_samples, nwords), where num_samples is the number\n",
    "# of text data samples in X_train.\n",
    "\n",
    "X_t = pad_sequences(token_trainer, maxlen=nwords)\n",
    "\n",
    "# Pads or truncates each sequence of integers to a fixed length of nwords and returns a numpy array of shape (num_samples, nwords), where num_samples is the number of text data samples in X_test\n",
    "X_te = pad_sequences(token_tester, maxlen=nwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699d10f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
